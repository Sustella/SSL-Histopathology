/home/users/rikiya/miniconda3/envs/transformer-ssl/lib/python3.9/site-packages/torch/distributed/launch.py:163: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
  logger.warn(
The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : moby_main.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:12345
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_ydfz519o/none_9al9jfie
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/home/users/rikiya/miniconda3/envs/transformer-ssl/lib/python3.9/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=12345
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_ydfz519o/none_9al9jfie/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_ydfz519o/none_9al9jfie/attempt_0/1/error.json
[W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
[W ProcessGroupNCCL.cpp:1569] Rank 1 using best-guess GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.
Traceback (most recent call last):
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/moby_main.py", line 242, in <module>
    main(config)
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/moby_main.py", line 83, in main
    dataset_train, data_loader_train, _ = build_loader(config)
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/data/build.py", line 29, in build_loader
Traceback (most recent call last):
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/moby_main.py", line 242, in <module>
    main(config)
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/moby_main.py", line 83, in main
    dataset_train, data_loader_train, _ = build_loader(config)
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/data/build.py", line 29, in build_loader
    dataset_train, config.MODEL.NUM_CLASSES = build_dataset(is_train=True, config=config)
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/data/build.py", line 82, in build_dataset
    dataset_train, config.MODEL.NUM_CLASSES = build_dataset(is_train=True, config=config)
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/data/build.py", line 82, in build_dataset
    transform = build_transform(is_train, config)
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/data/build.py", line 114, in build_transform
    transform = build_transform(is_train, config)
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/data/build.py", line 114, in build_transform
    T = stylize.StyleTransfer(style_dir=config.AUG.STRAP_STYLE_DIR, decoder_path=config.AUG.STRAP_DECODER_PATH, vgg_path=config.AUG.STRAP_VGG_PATH)
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/style_transfer/stylize.py", line 29, in __init__
    T = stylize.StyleTransfer(style_dir=config.AUG.STRAP_STYLE_DIR, decoder_path=config.AUG.STRAP_DECODER_PATH, vgg_path=config.AUG.STRAP_VGG_PATH)
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/style_transfer/stylize.py", line 29, in __init__
    assert len(styles) > 0, 'No images with specified extensions found in style directory' + style_dir
TypeError: can only concatenate str (not "PosixPath") to str
    assert len(styles) > 0, 'No images with specified extensions found in style directory' + style_dir
TypeError: can only concatenate str (not "PosixPath") to str
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 200423) of binary: /home/users/rikiya/miniconda3/envs/transformer-ssl/bin/python
ERROR:torch.distributed.elastic.agent.server.local_elastic_agent:[default] Worker group failed
INFO:torch.distributed.elastic.agent.server.api:[default] Worker group FAILED. 3/3 attempts left; will restart worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Stopping worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=1
  master_addr=127.0.0.1
  master_port=12345
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_ydfz519o/none_9al9jfie/attempt_1/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_ydfz519o/none_9al9jfie/attempt_1/1/error.json
slurmstepd: error: *** JOB 30541040 ON sh03-13n11 CANCELLED AT 2021-08-02T23:53:06 ***
Traceback (most recent call last):
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/moby_main.py", line 207, in <module>
Traceback (most recent call last):
  File "/home/users/rikiya/contrastive_strap/SSL-Transformer-Histopathology/Transformer-SSL/moby_main.py", line 207, in <module>
    torch.distributed.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)
  File "/home/users/rikiya/miniconda3/envs/transformer-ssl/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    torch.distributed.init_process_group(backend='nccl', init_method='env://', world_size=world_size, rank=rank)
  File "/home/users/rikiya/miniconda3/envs/transformer-ssl/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 547, in init_process_group
    _store_based_barrier(rank, store, timeout)
  File "/home/users/rikiya/miniconda3/envs/transformer-ssl/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 208, in _store_based_barrier
    _store_based_barrier(rank, store, timeout)
  File "/home/users/rikiya/miniconda3/envs/transformer-ssl/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 208, in _store_based_barrier
    worker_count = store.add(store_key, 0)
RuntimeError: Connection reset by peer
    worker_count = store.add(store_key, 0)
RuntimeError: Connection reset by peer
